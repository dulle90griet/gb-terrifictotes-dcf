# Data Warehouse Pipeline for TerrificTotes

![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/dulle90griet/gb-terrifictotes-dcf/deploy.yml)
![coverage](./docs/images/coverage-badge.svg) ![python](https://img.shields.io/badge/python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)

An automated, monitored ETL (Extract, Transform, Load) pipeline for fictional tote manufacturer TerrificTotes.

## ğŸŒ Overview

TerrificTotes's existing commercial and production systems store data in a write-optimized PostgreSQL database, unsuited to querying by analysts. `gb-terrifictotes-dcf` spins up a complete ecosystem of AWS cloud services in order to process new data from this database at regular intervals. Data is: 

- ingested for storage in JSON files; 
- denormalized and transformed into a new, OLAP-friendly schema, saved in parquet format; and finally 
- loaded into a data warehouse, ready for querying and integration into BI dashboards.

Change history is maintained from the moment of the pipeline's first operation. All stages are monitored, and basic error reporting triggers email notifications on system failure.

<img src="./docs/images/logos/logo_terraform.svg" width="32px" height="32px" alt="Terraform" /> ![S3](./docs/images/logos/Arch_Amazon-Simple-Storage-Service_32.png) ![Lambda](./docs/images/logos/Arch_AWS-Lambda_32.png) ![Secrets Manager](./docs/images/logos/Arch_AWS-Secrets-Manager_32.png) ![EventBridge](./docs/images/logos/Arch_Amazon-EventBridge_32.png) ![Step Functions](./docs/images/logos/Arch_AWS-Step-Functions_32.png) ![CloudWatch](./docs/images/logos/Arch_Amazon-CloudWatch_32.png) ![SNS](./docs/images/logos/Arch_Amazon-Simple-Notification-Service_32.png) ![RDS](./docs/images/logos/Arch_Amazon-RDS_32.png)

## ğŸ•¹ï¸ Demo

Every 15 minutes, an EventBridge scheduler begins a new pipeline execution. Any rows added to the origin database since the time of the last check are ingested into an S3 bucket as JSON packets, named with the start time of the current execution and arranged in folders according to source table.

![ingestion bucket](./docs/images/ingestion-bucket.png) ![ingested sales_order packets](./docs/images/ingestion-sales-order.png)

## ğŸ“œ Prerequisites

This project requires:

1. Python (3.9 <= version <= 3.12.4)

2. The [git CLI](https://git-scm.com/downloads)

3. [Terraform](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli) (developed using version 1.10.2)

4. An [AWS account](https://aws.amazon.com/free/)

5. [AWS credentials configured locally](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html), including access keys and default region

6. An [S3 bucket](https://aws.amazon.com/s3/) for remote storage of Terraform state files

7. A PostgreSQL OLTP database organized according to [the expected schema](./docs/images/oltp-db.png), accessible remotely via public IP or URL and receiving frequent ongoing updates

8. A second PostgreSQL database, accessible remotely via public IP or URL, which will be used for the data warehouse

## âš™ï¸ Setup

### ğŸ—ï¸ Project Setup

[Fork the repository](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo?platform=linux&tool=webui) on GitHub.

Clone it to your local system.

```sh
git clone https://github.com/YOUR-USERNAME-HERE/gb-terrifictotes-dcf
```

Change into the directory.

```sh
cd gb-terrifictotes-dcf
```

Install dependencies and set up the development environment.

```sh
make requirements && make dev-setup
```

### ğŸ” Secure Credentials Setup

[Create two AWS Secrets Manager secrets](https://docs.aws.amazon.com/secretsmanager/latest/userguide/hardcoded.html#hardcoded_step-1), both in the following format. In one secret, store credentials for the OLTP PSQL database. In the other, store credentials for the data warehouse.

```json
{
  "PG_USER": "YOUR_USERNAME_HERE",
  "PG_PASSWORD": "YOUR_PASSWORD_HERE",
  "PG_HOST":"YOUR_PSQL_IP_OR_URL_HERE",
  "PG_DATABASE":"YOUR_DATABASE_NAME_HERE",
  "PG_PORT":"5432"
}
```

In `src/ingestion_lambda.py`, update `connect_to_db()` with the name of the secret containing the OLTP credentials.

```python
credentials = retrieve_secret(sm_client, "YOUR-OLTP-SECRET-NAME-HERE")
```

In `src/uploading_lambda.py`, similarly update `connect_to_db()` with the name of the secret containing the data warehouse credentials.

```python
credentials = retrieve_secret(sm_client, "YOUR-DW-SECRET-NAME-HERE")
```

[Create three GitHub Actions secrets](https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions#creating-secrets-for-a-repository) to store the AWS credentials already used in your [project configuration](#-prerequisites), namely:

1. `AWS_ACCESS_KEY_ID`
2. `AWS_SECRET_ACCESS_KEY`
3. `AWS_REGION`

### ğŸŒ‹ Terraform Setup

In `terraform/main.tf`, update `backend "s3"` to refer to your [S3 remote state bucket](#-prequisites) and AWS region.

```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~>5.0"
    }
  }

  backend "s3" {
    bucket = "YOUR-BUCKET-NAME-HERE"
    key    = "terraform.tfstate"
    region = "YOUR-AWS-REGION-HERE"
  }
}
```

In `terraform/vars.tf`, first update `project-prefix` to a unique prefix for your fork's variables.

```hcl
variable "project_prefix" {
  type    = string
  default = "YOUR-CHOSEN-PREFIX-" 
}
```

Then update `error-alerts-recipient` to give the email address you'd like to receive automatic error notifications.

```hcl
variable "error-alerts-recipient" {
  type    = string
  default = "YOUR.EMAIL.ADDRESS@HERE.COM" 
}
```

### ğŸš€ Testing and Deployment

To run full checks, including safety, linting, testing and coverage, run:

```sh
make run-checks
```

If you wish to run individual tests â€“ unlikely to be necessary for the purposes of initial setup and deployment â€“ use the following command:

```sh
source ./venv/bin/activate && pytest -vvvrP test/TEST_FILE_NAME_HERE.py
```

To deploy the full AWS cloud pipeline using Terraform, first change into the `terraform` directory.

```sh
cd terraform
```

Initialise terraform, then plan and apply.

```sh
terraform init
terraform plan
terraform apply
```

Navigate to [Step Functions](https://console.aws.amazon.com/states) in the AWS Console, and click on the newly created state machine. Provided your databases are correctly set up and the IAM user associated with your credentials has all the necessary permissions, you should see a successful execution of the pipeline.

<p align="center"><img src="./docs/images/states-execution-succeeded.png" /></p>

Subsequent pushes to the `main` branch of the GitHub repo will trigger a CI/CD pipeline in GitHub Actions, once again linting, checking and testing the code and deploying any changes to AWS using `terraform apply`.

<p align="center"><img src="./docs/images/ci-cd-tests-and-deploy-succeeded.png" /></p>


## ğŸ«› Team GreenBeans

[`gb-terrifictotes-solutions`](https://github.com/dulle90griet/gb-terrifictotes-solutions) (ğŸ”’) was developed in November 2024 by [@Rmbkh](https://github.com/Rmkbh), [@dulle90griet](https://github.com/dulle90griet), [@contiele1](https://github.com/contiele1), [@tombracey](https://www.github.com/tombracey), [@ali-shep](https://github.com/ali-shep) and [@Minalpatil3](https://github.com/Minalpatil3), as their final project on the Northcoders Data Engineering Bootcamp.

[`gb-terrifictotes-dcf`](https://github.com/dulle90griet/gb-terrifictotes-dcf) is a comprehensive refactoring of that project by [@dulle90griet](https://github.com/dulle90griet). For an overview of current progress, [see below](#%EF%B8%8F-refactor-roadmap).

## ğŸ›£ï¸ Refactor Roadmap

- ğŸš› Create S3 backup tool for pipeline migration | âœ”ï¸ Dec 3 2024
- ğŸ”§ Create SQL script to initialize data warehouse | âœ”ï¸ Dec 4 2024
- ğŸ’š Fix CI build making unusable layer zip | âœ”ï¸ Dec 4 2024
- âœ… Add missing tests on ingestion functions | âœ”ï¸ Dec 11 2024
- â™»ï¸ Refactor and reorganise ingestion Lambda | âœ”ï¸ Dec 11 2024
- âœ… Add missing tests on processing functions | âœ”ï¸ Dec 14 2024
- â™»ï¸ Refactor and reorganise processing Lambda | âœ”ï¸ Dec 14 2024
- ğŸš§ Add missing tests on uploading functions | ğŸ‘·â€â™‚ï¸ In progress
- ğŸš§ Refactor and reorganise uploading Lambda | ğŸ‘·â€â™‚ï¸ In progress
- Establish consistency of logging
- Rationalize nomenclature
- Remove all deprecated code and modules

## Acknowledgements

- PSQL querying: [pg8000](https://github.com/tlocke/pg8000)
- Currency code conversions: [iso4217](https://github.com/dahlia/iso4217)
- Data manipulation and parquet formatting: [pandas](https://github.com/pandas-dev/pandas)